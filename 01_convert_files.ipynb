{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert input data (Jonathan D. MÃ¼ller & Rafat Qubaja)\n",
    "\n",
    "Loads the raw data of Rafat Qubaja's branch chambers, measured in Yatir, applies corrections and calculates fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization parameters\n",
    "\n",
    "# Data input\n",
    "project_path =  './'\n",
    "\n",
    "project_path_laser = project_path + '01_rawdata/laser computer/'\n",
    "project_path_tc    = project_path + '01_rawdata/thermocouples/'\n",
    "project_path_irga  = project_path + '01_rawdata/irga/'\n",
    "project_path_par   = project_path + '01_rawdata/PAR/'\n",
    "project_path_flow  = project_path + '01_rawdata/flow/'\n",
    "\n",
    "# Output\n",
    "project_path_output = project_path + '02_preprocessed_data/'\n",
    "\n",
    "# List of months to process\n",
    "# - If empty, all available data is processed\n",
    "# - Otherwise, specify using a string of year-month, e.g. ['2018-06'] or ['2017-03','2018-06']\n",
    "month_list = [ '2022-01' ]\n",
    "\n",
    "# Minimum IRGA pump flow\n",
    "# All gas data from the branch chamber IRGA data below this flow will be removed\n",
    "min_irga_flow = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main functions\n",
    "#---------------\n",
    "\n",
    "# Delete all relevant files in a folder.\n",
    "# - Used to remove 1h and 1min monthly files before running to prevent appending to existing files\n",
    "def empty_dir(directory):\n",
    "    files = glob.glob(directory + '*')\n",
    "    for f in files:\n",
    "        month_id = f[-10:-6] + '-' + f[-6:-4]\n",
    "        if(month_id in month_list):\n",
    "            print('Remove ' + f)\n",
    "            os.remove(f)\n",
    "        if(not month_list):\n",
    "            print('Remove ' + f)\n",
    "            os.remove(f)\n",
    "    pass\n",
    "\n",
    "# Read an Aerodyne laser input file, full file\n",
    "def read_laser_file(input_fn):\n",
    "    # Read data\n",
    "    data = [ ]\n",
    "    with open(input_fn) as f:\n",
    "        next(f) # Skip the first line\n",
    "        for line in f:\n",
    "            data.append(re.split(r'\\s', line.strip(), 9))\n",
    "    # build the generator        \n",
    "    for line in data:\n",
    "        if(len(line) < 9):\n",
    "            line.append('')\n",
    "    # first element returned is the columns\n",
    "    columns = ['timestamp','OCS.1','CO2.1','CO2.2','H2O.1','CO2.3','CO.1','OCS.2','CO2.4']\n",
    "    # build the data frame\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    #for index, row in df.iterrows():\n",
    "    #    df.loc[df['timestamp'] == row['timestamp'], 'timestamp'] = float(df.loc[df['timestamp'] == row['timestamp'], 'timestamp'])\n",
    "    df['timestamp'] = pd.to_numeric(df['timestamp'])\n",
    "    # Now apply normal conversions\n",
    "    df['timestamp'] = pd.to_datetime(df.timestamp, unit='s', origin=pd.Timestamp('1904-01-01')) - pd.DateOffset(seconds=1) # To convert IGOR-time (i.e. Excel 1904)\n",
    "    df['dayid'] = df['timestamp'].apply(lambda x:(x.year*10000 + x.month*100 + x.day))\n",
    "    return(df)\n",
    "\n",
    "# Read an Aerodyne laser input file, full file\n",
    "def read_laser_file_by_lines(input_fn, lines):\n",
    "    # Read data\n",
    "    data = [ ]\n",
    "    with open(input_fn) as f:\n",
    "        next(f) # Skip the first line\n",
    "        data.append(re.split(r'\\s', f.readline().strip(), 9))\n",
    "    # build the generator        \n",
    "    for line in data:\n",
    "        if(len(line) < 9):\n",
    "            line.append('')\n",
    "    # first element returned is the columns\n",
    "    columns = ['timestamp','OCS.1','CO2.1','CO2.2','H2O.1','CO2.3','CO.1','OCS.2','CO2.4']\n",
    "    # build the data frame\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    #for index, row in df.iterrows():\n",
    "    #    df.loc[df['timestamp'] == row['timestamp'], 'timestamp'] = float(df.loc[df['timestamp'] == row['timestamp'], 'timestamp'])\n",
    "    df['timestamp'] = pd.to_numeric(df['timestamp'])\n",
    "    # Now apply normal conversions\n",
    "    df['timestamp'] = pd.to_datetime(df.timestamp, unit='s', origin=pd.Timestamp('1904-01-01')) - pd.DateOffset(seconds=1) # To convert IGOR-time (i.e. Excel 1904)\n",
    "    df['dayid'] = df['timestamp'].apply(lambda x:(x.year*10000 + x.month*100 + x.day))\n",
    "    return(df)\n",
    "\n",
    "# OLD Read an Aerodyne laser input file, full file\n",
    "def read_laser_file_old(input_fn):\n",
    "    df = pd.read_csv(input_fn, sep=' ', skiprows=[0], index_col=False, header=0, names=['timestamp','OCS.1','CO2.1','CO2.2','H2O.1','CO2.3','CO.1','OCS.2','CO2.4'])\n",
    "    df['timestamp'] = pd.to_datetime(df.timestamp, unit='s', origin=pd.Timestamp('1904-01-01')) - pd.DateOffset(seconds=1) # To convert IGOR-time (i.e. Excel 1904)\n",
    "    df['dayid'] = df['timestamp'].apply(lambda x:(x.year*10000 + x.month*100 + x.day))\n",
    "    return(df)\n",
    "\n",
    "# OLD Read an Aerodyne laser input file, full file\n",
    "def read_laser_file_by_lines_old(input_fn, lines):\n",
    "    df = pd.read_csv(input_fn, sep=' ', skiprows=[0], nrows=lines, index_col=False, header=0, names=['timestamp','OCS.1','CO2.1','CO2.2','H2O.1','CO2.3','CO.1','OCS.2','CO2.4'])\n",
    "    df['timestamp'] = pd.to_datetime(df.timestamp, unit='s', origin=pd.Timestamp('1904-01-01')) - pd.DateOffset(seconds=1) # To convert IGOR-time (i.e. Excel 1904)\n",
    "    df['dayid'] = df['timestamp'].apply(lambda x:(x.year*10000 + x.month*100 + x.day))\n",
    "    return(df)\n",
    "\n",
    "# Read a CR1000 input file, full file\n",
    "def read_cr1000_file(input_fn):\n",
    "    df = pd.read_csv(input_fn,skiprows=[0,2,3,4,5], na_values=[\"NAN\"])\n",
    "    if(df.columns[0] != 'TIMESTAMP'):\n",
    "        df = pd.read_csv(input_fn,skiprows=[0,1,3,4,5,6], na_values=[\"NAN\"])\n",
    "    df.rename(columns={'TIMESTAMP':'timestamp'}, inplace=True)\n",
    "    df['timestamp'] = pd.to_datetime( df.timestamp, format='%Y-%m-%d %H:%M:%S', utc=True, errors=\"raise\")#errors='coerce')\n",
    "    df['dayid'] = df['timestamp'].apply(lambda x:(x.year*10000 + x.month*100 + x.day))\n",
    "    return(df)\n",
    "\n",
    "# Read a CR1000 input file by lines, only some initial lines\n",
    "def read_cr1000_file_by_lines(input_fn, lines):\n",
    "    df = pd.read_csv(input_fn,skiprows=[0,2,3,4], na_values=[\"NAN\"],nrows=lines)\n",
    "    if(df.columns[0] != 'TIMESTAMP'):\n",
    "        df = pd.read_csv(input_fn,skiprows=[0,1,3,4,5], na_values=[\"NAN\"],nrows=lines)\n",
    "    df.rename(columns={'TIMESTAMP':'timestamp'}, inplace=True)\n",
    "    df['timestamp'] = pd.to_datetime( df.timestamp, format='%Y-%m-%d %H:%M:%S', utc=True, errors=\"raise\")#errors='coerce')\n",
    "    df['dayid'] = df['timestamp'].apply(lambda x:(x.year*10000 + x.month*100 + x.day))\n",
    "    return(df)\n",
    "\n",
    "# Read a CR1000 input file, full file\n",
    "def read_irga_file(input_fn):\n",
    "    df = pd.read_csv(input_fn,skiprows=[0,2,3,4,5], na_values=[\"NAN\"])\n",
    "    if(df.columns[0] != 'TIMESTAMP'):\n",
    "        df = pd.read_csv(input_fn,skiprows=[0,1,3,4,5,6], na_values=[\"NAN\"])\n",
    "    df.rename(columns={'TIMESTAMP':'timestamp'}, inplace=True)\n",
    "    df['timestamp'] = pd.to_datetime( df.timestamp, format='%Y-%m-%d %H:%M:%S', utc=True, errors=\"raise\")#errors='coerce')\n",
    "    df['dayid'] = df['timestamp'].apply(lambda x:(x.year*10000 + x.month*100 + x.day))\n",
    "    return(df)\n",
    "\n",
    "# Read a CR1000 input file by lines\n",
    "def read_irga_file_by_lines(input_fn, lines):\n",
    "    df = pd.read_csv(input_fn,skiprows=[0,2,3,4], na_values=[\"NAN\"],nrows=lines)\n",
    "    if(df.columns[0] != 'TIMESTAMP'):\n",
    "        df = pd.read_csv(input_fn,skiprows=[0,1,3,4,5], na_values=[\"NAN\"],nrows=lines)\n",
    "    df.rename(columns={'TIMESTAMP':'timestamp'}, inplace=True)\n",
    "    df['timestamp'] = pd.to_datetime( df.timestamp, format='%Y-%m-%d %H:%M:%S', utc=True, errors=\"raise\")#errors='coerce')\n",
    "    df['dayid'] = df['timestamp'].apply(lambda x:(x.year*10000 + x.month*100 + x.day))\n",
    "    return(df)\n",
    "\n",
    "def remove_obsolete_irga_data(temp):\n",
    "    temp = temp.copy()\n",
    "    \n",
    "    # Rename PAR & temperature\n",
    "    temp.rename(columns={'RadKipZonen':'par.ambient.umol_m2_s'}, inplace=True)\n",
    "    temp.rename(columns={'Tc(8)':'temp.air.ambient.c'}, inplace=True)\n",
    "    temp.rename(columns={'H2o_6262_mmol_mol':'h2o.irga.ambient.mmol_mol'}, inplace=True)\n",
    "    temp.rename(columns={'Co2_6262_micmol_mol':'co2.irga.ambient.umol_mol'}, inplace=True)\n",
    "    temp.rename(columns={'AirFlow_Amb':'pump.flow.irga.lpm'}, inplace=True)\n",
    "    temp.rename(columns={'Prees_7000':'P.irga.kPa'}, inplace=True)\n",
    "    \n",
    "    # Remove bad data\n",
    "    temp.loc[temp['pump.flow.irga.lpm'] <= min_irga_flow, 'h2o.irga.ambient.mmol_mol'] = np.nan\n",
    "    temp.loc[temp['pump.flow.irga.lpm'] <= min_irga_flow, 'co2.irga.ambient.umol_mol'] = np.nan\n",
    "    \n",
    "    # Keep only relevant columns\n",
    "    temp = temp[['timestamp','temp.air.ambient.c','par.ambient.umol_m2_s','h2o.irga.ambient.mmol_mol','co2.irga.ambient.umol_mol','P.irga.kPa','dayid']]\n",
    "    \n",
    "    return(temp)\n",
    "\n",
    "# Writes output files in full temporal resolution\n",
    "def write_output_file(out_df, date_idx, out_dir, output_fn):\n",
    "    out_df = out_df.copy()\n",
    "    # Drop duplicates\n",
    "    out_df.drop_duplicates(subset = 'timestamp', inplace=True)\n",
    "    # Sort, in case it's not yet the case\n",
    "    out_df.sort_values('timestamp', inplace=True)\n",
    "    \n",
    "    # Check if output folders exist. If not, create\n",
    "    month_dir = str(date_idx)[0:4] + \"-\" + str(date_idx)[4:6]\n",
    "    if(not os.path.exists(out_dir + month_dir)): # make directory if it doesn't exist\n",
    "        os.makedirs(out_dir + month_dir)\n",
    "    # Create file name\n",
    "    out_fn = out_dir + month_dir + \"/\" + output_fn + \"_\" + str(date_idx) + \".csv\"\n",
    "    #print(str(len(out_df.dayid)), out_fn) # Shows final file size)\n",
    "    # organise data for output\n",
    "    temp_df = out_df\n",
    "    # Before saving, remove the index\n",
    "    temp_df.drop('dayid', axis=1, inplace=True)\n",
    "    # Move timestamp column to the front\n",
    "    col = temp_df.pop('timestamp')\n",
    "    temp_df.insert(0, col.name, col, allow_duplicates=True)\n",
    "    # Remove timezone information\n",
    "    temp_df['timestamp'] = temp_df['timestamp'].dt.tz_localize(None)\n",
    "    # Write data\n",
    "    temp_df.to_csv(out_fn, index=False, encoding='utf-8', date_format='%Y-%m-%d %H:%M:%S') # Save file\n",
    "    \n",
    "# Creates a daily file for each type\n",
    "def organise_files(project_path, output_path, filetype):\n",
    "    # List all files in the directories\n",
    "    fn_list = sorted(glob.glob(project_path + '*/*', recursive=True))\n",
    "    if(filetype == 'laser'):\n",
    "        fn_list = sorted(glob.glob(project_path + '*/*.str', recursive=True))\n",
    "    saved = []\n",
    "    \n",
    "    # Create output path name\n",
    "    project_path_output = output_path + project_path.split('/')[-2] + '/'\n",
    "\n",
    "    # For all files in the directory\n",
    "    for fn_i, fn in enumerate(fn_list):\n",
    "        # Only run data in the month list\n",
    "        current_month = fn.replace(project_path[:-1], \"\")[1:8]\n",
    "        if((current_month in month_list) or (len(month_list) == 0)):\n",
    "            if( (filetype == 'irga') & (int(current_month.replace('-','')) < 202007) ):\n",
    "                continue\n",
    "            #display(fn.replace(project_path_chambers[:-1], \"\")[1:8])\n",
    "            pass\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Debugging message\n",
    "        if(fn_i % 1 == 0): # % 20 to show every 20th file being loaded\n",
    "            print( '{:<07}'.format(str(round(fn_i * 100 / len(fn_list), 4))) + \"%\\t\\t\" + fn.split('\\\\')[-2] + '/' + fn.split('\\\\')[-1]) # Show status\n",
    "            #print( '{:<07}'.format(str(round(fn_i * 100 / len(fn_list), 4))) + \"%\\t\\t\" + fn.replace(project_path + \"01_rawdata/\", \"\")) # Show status\n",
    "    \n",
    "        # Load the current laser file\n",
    "        if(filetype == 'laser'):\n",
    "            df = read_laser_file(fn)\n",
    "        else:\n",
    "            df = read_cr1000_file(fn)\n",
    "            if(filetype == 'irga'):\n",
    "                df = remove_obsolete_irga_data(df)\n",
    "                project_path_output = output_path + 'ambient/'\n",
    "    \n",
    "        if (fn_i != len(fn_list)-1):\n",
    "            # Load next file\n",
    "            if(filetype == 'laser'):\n",
    "                df_next = read_laser_file_by_lines(fn_list[fn_i+1], lines=1)\n",
    "            else:\n",
    "                df_next = read_cr1000_file_by_lines(fn_list[fn_i+1], lines=1)\n",
    "            next_day = df_next['dayid'].tolist()[0]\n",
    "            final_file = False\n",
    "        else:\n",
    "            next_day = 0\n",
    "            final_file = True\n",
    "  \n",
    "        # Group by per-day id\n",
    "        grouped = df.groupby(['dayid'])\n",
    "        #print(\"    Days in this file:  \", len(grouped))\n",
    "    \n",
    "        # For each group\n",
    "        for group_i, (this_day, day) in enumerate(grouped):\n",
    "        \n",
    "            # If we have saved data and if the day matches append the current\n",
    "            if (len(saved) > 0):\n",
    "                if (this_day == saved['dayid'].tolist()[0]):\n",
    "                    day = pd.concat([saved, day], axis=0, ignore_index=True)\n",
    "                    #print(\"    Current line count: \", str(len(day.dayid)))\n",
    "        \n",
    "            #If this is the final group in the file and this is not the final file and the first group of the next file is the same day\n",
    "            if (group_i == len(grouped)-1) and (not final_file) and (this_day == next_day):\n",
    "                saved = day\n",
    "                continue  \n",
    "            else:\n",
    "                write_output_file(day, this_day, project_path_output, filetype)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert files to daily output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laser files\n",
      "0.00000%\t\t2022-01/220110_000001.str\n",
      "14.2857%\t\t2022-01/220111_000001.str\n",
      "28.5714%\t\t2022-01/220112_000001.str\n",
      "42.8571%\t\t2022-01/220113_000001.str\n",
      "57.1429%\t\t2022-01/220113_110229.str\n",
      "71.4286%\t\t2022-01/220114_000001.str\n",
      "85.7143%\t\t2022-01/220114_150701.str\n",
      "Done...\n"
     ]
    }
   ],
   "source": [
    "# Run calculations\n",
    "#-----------------\n",
    "\n",
    "print('Laser files')\n",
    "organise_files(project_path_laser, project_path_output, 'laser')\n",
    "\n",
    "print(\"Done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAR files\n",
      "0.00000%\t\t2022-01/CR1000_SF_PARdata_2022_01_10_0001.dat\n",
      "20.0000%\t\t2022-01/CR1000_SF_PARdata_2022_01_11_0001.dat\n",
      "40.0000%\t\t2022-01/CR1000_SF_PARdata_2022_01_12_0001.dat\n",
      "60.0000%\t\t2022-01/CR1000_SF_PARdata_2022_01_13_0001.dat\n",
      "80.0000%\t\t2022-01/CR1000_SF_PARdata_2022_01_14_0001.dat\n",
      "Done...\n"
     ]
    }
   ],
   "source": [
    "print('PAR files')\n",
    "organise_files(project_path_par, project_path_output, 'par')\n",
    "\n",
    "print(\"Done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thermocouple files\n",
      "0.00000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_10_0001.dat\n",
      "0.83330%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_10_0101.dat\n",
      "1.66670%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_10_0201.dat\n",
      "2.50000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_10_0301.dat\n",
      "3.33330%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_10_0401.dat\n",
      "4.16670%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_10_0501.dat\n",
      "5.00000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_10_0601.dat\n",
      "5.83330%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_10_0701.dat\n",
      "6.66670%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_10_0801.dat\n",
      "7.50000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_10_0901.dat\n",
      "8.33330%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_10_1001.dat\n",
      "9.16670%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_10_1101.dat\n",
      "10.0000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_10_1201.dat\n",
      "10.8333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_10_1301.dat\n",
      "11.6667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_10_1401.dat\n",
      "12.5000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_10_1501.dat\n",
      "13.3333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_10_1601.dat\n",
      "14.1667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_10_1701.dat\n",
      "15.0000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_10_1801.dat\n",
      "15.8333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_10_1901.dat\n",
      "16.6667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_10_2001.dat\n",
      "17.5000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_10_2101.dat\n",
      "18.3333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_10_2201.dat\n",
      "19.1667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_10_2301.dat\n",
      "20.0000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_11_0001.dat\n",
      "20.8333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_11_0101.dat\n",
      "21.6667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_11_0201.dat\n",
      "22.5000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_11_0301.dat\n",
      "23.3333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_11_0401.dat\n",
      "24.1667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_11_0501.dat\n",
      "25.0000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_11_0601.dat\n",
      "25.8333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_11_0701.dat\n",
      "26.6667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_11_0801.dat\n",
      "27.5000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_11_0901.dat\n",
      "28.3333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_11_1001.dat\n",
      "29.1667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_11_1101.dat\n",
      "30.0000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_11_1201.dat\n",
      "30.8333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_11_1301.dat\n",
      "31.6667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_11_1401.dat\n",
      "32.5000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_11_1501.dat\n",
      "33.3333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_11_1601.dat\n",
      "34.1667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_11_1701.dat\n",
      "35.0000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_11_1801.dat\n",
      "35.8333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_11_1901.dat\n",
      "36.6667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_11_2001.dat\n",
      "37.5000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_11_2101.dat\n",
      "38.3333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_11_2201.dat\n",
      "39.1667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_11_2301.dat\n",
      "40.0000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_12_0001.dat\n",
      "40.8333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_12_0101.dat\n",
      "41.6667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_12_0201.dat\n",
      "42.5000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_12_0301.dat\n",
      "43.3333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_12_0401.dat\n",
      "44.1667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_12_0501.dat\n",
      "45.0000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_12_0601.dat\n",
      "45.8333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_12_0701.dat\n",
      "46.6667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_12_0801.dat\n",
      "47.5000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_12_0901.dat\n",
      "48.3333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_12_1001.dat\n",
      "49.1667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_12_1101.dat\n",
      "50.0000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_12_1201.dat\n",
      "50.8333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_12_1301.dat\n",
      "51.6667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_12_1401.dat\n",
      "52.5000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_12_1501.dat\n",
      "53.3333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_12_1601.dat\n",
      "54.1667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_12_1701.dat\n",
      "55.0000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_12_1800.dat\n",
      "55.8333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_12_1900.dat\n",
      "56.6667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_12_2000.dat\n",
      "57.5000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_12_2100.dat\n",
      "58.3333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_12_2200.dat\n",
      "59.1667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_12_2300.dat\n",
      "60.0000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_13_0000.dat\n",
      "60.8333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_13_0100.dat\n",
      "61.6667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_13_0200.dat\n",
      "62.5000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_13_0300.dat\n",
      "63.3333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_13_0400.dat\n",
      "64.1667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_13_0501.dat\n",
      "65.0000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_13_0601.dat\n",
      "65.8333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_13_0701.dat\n",
      "66.6667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_13_0801.dat\n",
      "67.5000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_13_0901.dat\n",
      "68.3333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_13_1001.dat\n",
      "69.1667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_13_1100.dat\n",
      "70.0000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_13_1200.dat\n",
      "70.8333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_13_1301.dat\n",
      "71.6667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_13_1401.dat\n",
      "72.5000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_13_1501.dat\n",
      "73.3333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_13_1601.dat\n",
      "74.1667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_13_1701.dat\n",
      "75.0000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_13_1801.dat\n",
      "75.8333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_13_1901.dat\n",
      "76.6667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_13_2001.dat\n",
      "77.5000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_13_2101.dat\n",
      "78.3333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_13_2201.dat\n",
      "79.1667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_13_2301.dat\n",
      "80.0000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_14_0001.dat\n",
      "80.8333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_14_0101.dat\n",
      "81.6667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_14_0201.dat\n",
      "82.5000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_14_0301.dat\n",
      "83.3333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_14_0401.dat\n",
      "84.1667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_14_0501.dat\n",
      "85.0000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_14_0601.dat\n",
      "85.8333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_14_0701.dat\n",
      "86.6667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_14_0801.dat\n",
      "87.5000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_14_0901.dat\n",
      "88.3333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_14_1001.dat\n",
      "89.1667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_14_1101.dat\n",
      "90.0000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_14_1201.dat\n",
      "90.8333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_14_1301.dat\n",
      "91.6667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_14_1401.dat\n",
      "92.5000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_14_1501.dat\n",
      "93.3333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_14_1601.dat\n",
      "94.1667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_14_1701.dat\n",
      "95.0000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_14_1801.dat\n",
      "95.8333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_14_1901.dat\n",
      "96.6667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_14_2001.dat\n",
      "97.5000%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_14_2101.dat\n",
      "98.3333%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_14_2201.dat\n",
      "99.1667%\t\t2022-01/CR1000_Chambers_TcrafatData_2022_01_14_2301.dat\n",
      "Done...\n"
     ]
    }
   ],
   "source": [
    "print('Thermocouple files')\n",
    "organise_files(project_path_tc, project_path_output, 'tc')\n",
    "\n",
    "print(\"Done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow rate files\n",
      "0.00000%\t\t2022-01/CR1000_Chambers_FlowData_02_12_2021-22_06_2022.dat\n",
      "Done...\n"
     ]
    }
   ],
   "source": [
    "print('Flow rate files')\n",
    "organise_files(project_path_flow, project_path_output, 'flow')\n",
    "\n",
    "print(\"Done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRGA files\n",
      "0.00000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_10_0000.dat\n",
      "0.83330%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_10_0100.dat\n",
      "1.66670%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_10_0200.dat\n",
      "2.50000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_10_0300.dat\n",
      "3.33330%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_10_0400.dat\n",
      "4.16670%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_10_0500.dat\n",
      "5.00000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_10_0600.dat\n",
      "5.83330%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_10_0700.dat\n",
      "6.66670%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_10_0800.dat\n",
      "7.50000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_10_0900.dat\n",
      "8.33330%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_10_1000.dat\n",
      "9.16670%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_10_1100.dat\n",
      "10.0000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_10_1200.dat\n",
      "10.8333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_10_1300.dat\n",
      "11.6667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_10_1400.dat\n",
      "12.5000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_10_1500.dat\n",
      "13.3333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_10_1600.dat\n",
      "14.1667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_10_1700.dat\n",
      "15.0000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_10_1800.dat\n",
      "15.8333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_10_1900.dat\n",
      "16.6667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_10_2000.dat\n",
      "17.5000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_10_2100.dat\n",
      "18.3333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_10_2200.dat\n",
      "19.1667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_10_2300.dat\n",
      "20.0000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_11_0000.dat\n",
      "20.8333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_11_0100.dat\n",
      "21.6667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_11_0200.dat\n",
      "22.5000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_11_0300.dat\n",
      "23.3333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_11_0400.dat\n",
      "24.1667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_11_0500.dat\n",
      "25.0000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_11_0600.dat\n",
      "25.8333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_11_0700.dat\n",
      "26.6667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_11_0800.dat\n",
      "27.5000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_11_0900.dat\n",
      "28.3333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_11_1000.dat\n",
      "29.1667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_11_1100.dat\n",
      "30.0000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_11_1200.dat\n",
      "30.8333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_11_1300.dat\n",
      "31.6667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_11_1400.dat\n",
      "32.5000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_11_1500.dat\n",
      "33.3333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_11_1600.dat\n",
      "34.1667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_11_1700.dat\n",
      "35.0000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_11_1800.dat\n",
      "35.8333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_11_1900.dat\n",
      "36.6667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_11_2000.dat\n",
      "37.5000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_11_2100.dat\n",
      "38.3333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_11_2200.dat\n",
      "39.1667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_11_2300.dat\n",
      "40.0000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_12_0000.dat\n",
      "40.8333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_12_0100.dat\n",
      "41.6667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_12_0200.dat\n",
      "42.5000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_12_0300.dat\n",
      "43.3333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_12_0400.dat\n",
      "44.1667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_12_0500.dat\n",
      "45.0000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_12_0600.dat\n",
      "45.8333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_12_0700.dat\n",
      "46.6667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_12_0800.dat\n",
      "47.5000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_12_0900.dat\n",
      "48.3333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_12_1000.dat\n",
      "49.1667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_12_1100.dat\n",
      "50.0000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_12_1200.dat\n",
      "50.8333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_12_1300.dat\n",
      "51.6667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_12_1400.dat\n",
      "52.5000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_12_1402.dat\n",
      "53.3333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_12_1600.dat\n",
      "54.1667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_12_1700.dat\n",
      "55.0000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_12_1800.dat\n",
      "55.8333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_12_1900.dat\n",
      "56.6667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_12_2000.dat\n",
      "57.5000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_12_2100.dat\n",
      "58.3333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_12_2200.dat\n",
      "59.1667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_12_2300.dat\n",
      "60.0000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_13_0000.dat\n",
      "60.8333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_13_0100.dat\n",
      "61.6667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_13_0200.dat\n",
      "62.5000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_13_0300.dat\n",
      "63.3333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_13_0400.dat\n",
      "64.1667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_13_0500.dat\n",
      "65.0000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_13_0600.dat\n",
      "65.8333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_13_0700.dat\n",
      "66.6667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_13_0800.dat\n",
      "67.5000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_13_0900.dat\n",
      "68.3333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_13_1000.dat\n",
      "69.1667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_13_1100.dat\n",
      "70.0000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_13_1200.dat\n",
      "70.8333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_13_1300.dat\n",
      "71.6667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_13_1400.dat\n",
      "72.5000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_13_1500.dat\n",
      "73.3333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_13_1600.dat\n",
      "74.1667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_13_1700.dat\n",
      "75.0000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_13_1800.dat\n",
      "75.8333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_13_1900.dat\n",
      "76.6667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_13_2000.dat\n",
      "77.5000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_13_2100.dat\n",
      "78.3333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_13_2200.dat\n",
      "79.1667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_13_2300.dat\n",
      "80.0000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_14_0000.dat\n",
      "80.8333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_14_0100.dat\n",
      "81.6667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_14_0200.dat\n",
      "82.5000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_14_0300.dat\n",
      "83.3333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_14_0400.dat\n",
      "84.1667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_14_0500.dat\n",
      "85.0000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_14_0600.dat\n",
      "85.8333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_14_0700.dat\n",
      "86.6667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_14_0800.dat\n",
      "87.5000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_14_0900.dat\n",
      "88.3333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_14_1000.dat\n",
      "89.1667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_14_1100.dat\n",
      "90.0000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_14_1200.dat\n",
      "90.8333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_14_1300.dat\n",
      "91.6667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_14_1400.dat\n",
      "92.5000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_14_1500.dat\n",
      "93.3333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_14_1600.dat\n",
      "94.1667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_14_1700.dat\n",
      "95.0000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_14_1800.dat\n",
      "95.8333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_14_1900.dat\n",
      "96.6667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_14_2000.dat\n",
      "97.5000%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_14_2100.dat\n",
      "98.3333%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_14_2200.dat\n",
      "99.1667%\t\t2022-01/CR1000_Chambers_Data_dropbox_2022_01_14_2300.dat\n",
      "Done...\n"
     ]
    }
   ],
   "source": [
    "# IRGA files needed for the ambient data. There is a lot, so it's slow...\n",
    "print('IRGA files')\n",
    "organise_files(project_path_irga, project_path_output, 'irga')\n",
    "\n",
    "print(\"Done...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing data replacement\n",
    "\n",
    "There were gaps in the measurements due to device failure. Data was considered to be stable during this time period and was therefore replaced as follows:\n",
    "\n",
    "- FR gap:    [started at 11th July 2021 end 7th Aug 2021], we will use 10th July 2021 for July days, and 9th Aug 2021 for Aug days  \n",
    "- Tc gap:    [started at 13th July 2021 end 23rd July 2021], we will use 11th July 2021 for July days\n",
    "- PAR gap:   [started at 11th July 2021 end 7th Aug 2021], we will use 10th July 2021 for July days, and 8th Aug 2021 for Aug days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./02_preprocessed_data/thermocouples/2021-07/tc_20210711.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './02_preprocessed_data/thermocouples/2021-07/tc_20210711.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e2d861319120>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0minput_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproject_path_output\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'thermocouples/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'2021-07/tc_20210711.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0minput_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0minput_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'timestamp'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'%Y-%m-%d %H:%M:%S'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './02_preprocessed_data/thermocouples/2021-07/tc_20210711.csv'"
     ]
    }
   ],
   "source": [
    "# Replace bad Tc, July 2021\n",
    "\n",
    "# Read input file\n",
    "input_fn = project_path_output + 'thermocouples/' + '2021-07/tc_20210711.csv'\n",
    "print(input_fn)\n",
    "input_df = pd.read_csv(input_fn)\n",
    "input_df['timestamp'] = pd.to_datetime(input_df.timestamp, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Calculate number of days offset\n",
    "days_offset = str(input_df['timestamp'].dt.day.values[0] - 1) + 'days'\n",
    "current_month = input_df['timestamp'].dt.month.values[0]\n",
    "input_df['timestamp'] = input_df['timestamp'] - pd.Timedelta(days_offset)\n",
    "\n",
    "# July\n",
    "for i in np.arange(14, 23):\n",
    "    out_df = input_df.copy()\n",
    "    out_df['timestamp'] = out_df['timestamp'] + pd.Timedelta(str(i-1) + 'day')\n",
    "    out_df['dayid'] = 2021*10000 + current_month*100 + i\n",
    "    date_idx = 2021*10000 + current_month*100 + i\n",
    "    print(date_idx)\n",
    "    write_output_file(out_df, date_idx, project_path_output + 'thermocouples/', 'tc')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace bad flow, July 2021\n",
    "\n",
    "# Read input file\n",
    "input_fn = project_path_output + 'flow/' + '2021-07/flow_20210710.csv'\n",
    "print(input_fn)\n",
    "input_df = pd.read_csv(input_fn)\n",
    "input_df['timestamp'] = pd.to_datetime(input_df.timestamp, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Calculate number of days offset\n",
    "days_offset = str(input_df['timestamp'].dt.day.values[0] - 1) + 'days'\n",
    "current_month = input_df['timestamp'].dt.month.values[0]\n",
    "input_df['timestamp'] = input_df['timestamp'] - pd.Timedelta(days_offset)\n",
    "\n",
    "# July\n",
    "for i in np.arange(11, 32):\n",
    "    out_df = input_df.copy()\n",
    "    out_df['timestamp'] = out_df['timestamp'] + pd.Timedelta(str(i-1) + 'day')\n",
    "    out_df['dayid'] = 2021*10000 + current_month*100 + i\n",
    "    date_idx = 2021*10000 + current_month*100 + i\n",
    "    print(date_idx)\n",
    "    write_output_file(out_df, date_idx, project_path_output + 'flow/', 'flow')\n",
    "    pass\n",
    "\n",
    "# Replace bad flow, Aug 2021\n",
    "\n",
    "# Read input file\n",
    "input_fn = project_path_output + 'flow/' + '2021-08/flow_20210809.csv'\n",
    "print(input_fn)\n",
    "input_df = pd.read_csv(input_fn)\n",
    "input_df['timestamp'] = pd.to_datetime(input_df.timestamp, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Calculate number of days offset\n",
    "days_offset = str(input_df['timestamp'].dt.day.values[0] - 1) + 'days'\n",
    "current_month = input_df['timestamp'].dt.month.values[0]\n",
    "input_df['timestamp'] = input_df['timestamp'] - pd.Timedelta(days_offset)\n",
    "\n",
    "# July\n",
    "for i in np.arange(1, 8):\n",
    "    out_df = input_df.copy()\n",
    "    out_df['timestamp'] = out_df['timestamp'] + pd.Timedelta(str(i-1) + 'day')\n",
    "    out_df['dayid'] = 2021*10000 + current_month*100 + i\n",
    "    date_idx = 2021*10000 + current_month*100 + i\n",
    "    print(date_idx)\n",
    "    write_output_file(out_df, date_idx, project_path_output + 'flow/', 'flow')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace bad PAR, July 2021\n",
    "\n",
    "# Read input file\n",
    "input_fn = project_path_output + 'PAR/' + '2021-07/par_20210710.csv'\n",
    "print(input_fn)\n",
    "input_df = pd.read_csv(input_fn)\n",
    "input_df['timestamp'] = pd.to_datetime(input_df.timestamp, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Calculate number of days offset\n",
    "days_offset = str(input_df['timestamp'].dt.day.values[0] - 1) + 'days'\n",
    "current_month = input_df['timestamp'].dt.month.values[0]\n",
    "input_df['timestamp'] = input_df['timestamp'] - pd.Timedelta(days_offset)\n",
    "\n",
    "# July\n",
    "for i in np.arange(12, 32):\n",
    "    out_df = input_df.copy()\n",
    "    out_df['timestamp'] = out_df['timestamp'] + pd.Timedelta(str(i-1) + 'day')\n",
    "    out_df['dayid'] = 2021*10000 + current_month*100 + i\n",
    "    date_idx = 2021*10000 + current_month*100 + i\n",
    "    print(date_idx)\n",
    "    write_output_file(out_df, date_idx, project_path_output + 'PAR/', 'par')\n",
    "    pass\n",
    "\n",
    "# Replace bad PAR, Aug 2021\n",
    "\n",
    "# Read input file\n",
    "input_fn = project_path_output + 'PAR/' + '2021-08/par_20210808.csv'\n",
    "print(input_fn)\n",
    "input_df = pd.read_csv(input_fn)\n",
    "input_df['timestamp'] = pd.to_datetime(input_df.timestamp, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Calculate number of days offset\n",
    "days_offset = str(input_df['timestamp'].dt.day.values[0] - 1) + 'days'\n",
    "current_month = input_df['timestamp'].dt.month.values[0]\n",
    "input_df['timestamp'] = input_df['timestamp'] - pd.Timedelta(days_offset)\n",
    "\n",
    "# July\n",
    "for i in np.arange(1, 8):\n",
    "    out_df = input_df.copy()\n",
    "    out_df['timestamp'] = out_df['timestamp'] + pd.Timedelta(str(i-1) + 'day')\n",
    "    out_df['dayid'] = 2021*10000 + current_month*100 + i\n",
    "    date_idx = 2021*10000 + current_month*100 + i\n",
    "    print(date_idx)\n",
    "    #display(out_df)\n",
    "    write_output_file(out_df, date_idx, project_path_output + 'PAR/', 'par')\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
